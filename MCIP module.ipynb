{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define and write the module as a .py file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile mcip.py\n",
    "\n",
    "import numpy as np\n",
    "from IPython.display import clear_output\n",
    "import itertools as it\n",
    "import pandas as pd\n",
    "from tqdm import tqdm_notebook\n",
    "from sklearn.neighbors import BallTree\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def pipelineOfVariation(caseInd, dfTrain, dfTest, printOutput, tolerance_Value, categorical, \n",
    "                        continuous, radius, alpha, variations, partialLinear, linearVarCols):\n",
    "    \"\"\"\n",
    "    Method that perfroms a pipeline of imputing the data and returns imputed data\n",
    "    caseInd: index of test case\n",
    "    dfTrain: training set dataframe\n",
    "    dfTest:test set dataframe\n",
    "    printOutput: if for one case maybe want o print the ouput of number if similar cases\n",
    "    tolerance_Value: represents the tolerance value\n",
    "    categorical: list of categorical variables\n",
    "    continuous: list of continuous variables\n",
    "    radius: scalar that defines the radius of the hypersphere of the BallTree algorithm used to find similar cases\n",
    "    alpha: the confidence interval value\n",
    "    variations: boolean that represents whether to generate C.I.s for imputations\n",
    "    partialLinear: boolean that represents whether we generate simulated data points in uncertain values\n",
    "    linearVarCols: list of columns fo the variables to generate simulated data points in uncertain values\n",
    "    \n",
    "    Retruns:\n",
    "    the imputed test case, in the case of missing continuous variables it returns the possible imputations of these continuous variables\n",
    "    \"\"\"\n",
    "    \n",
    "    nullMatrix = dfTest.isnull().as_matrix()\n",
    "    row = nullMatrix[caseInd,:]\n",
    "    combs = getCombinations(row,dfTrain,tolerance_Value=tolerance_Value)\n",
    "    \n",
    "    if sum(row)>0: #if there are missing values\n",
    "        dfAllNNs, _ , _ = getNNs(dfTrain, dfTest, combs, row, radius=radius, printOutput=printOutput, caseInd=caseInd)\n",
    "    else:\n",
    "        dfAllNNs = None\n",
    "        \n",
    "    if np.sum(row)==0:\n",
    "        x = getDatasetOnlyVariationsLinear(dfTest=dfTest, row=row, caseInd=caseInd, categorical=categorical, \n",
    "                                   continuous=continuous, variations=variations, \n",
    "                                   partialLinear=partialLinear,linearVarCols = linearVarCols)\n",
    "    else:\n",
    "        x = getDatasetOfVariations(dfAllNNs, dfTest, row=row,caseInd=caseInd, categorical=categorical, \n",
    "                                   continuous=continuous, alpha=alpha, variations=variations, \n",
    "                                   partialLinear=partialLinear,linearVarCols = linearVarCols)\n",
    "    \n",
    "    if printOutput==False:\n",
    "        clear_output()\n",
    "    \n",
    "    return x\n",
    "    \n",
    "\n",
    "\n",
    "def queryNN(X_train, X_test, radius, leaf_size):\n",
    "    \"\"\"\n",
    "    Method that identifies from a dataset the NN most similar cases (Nearest neighbors).\n",
    "    X_train: dataset to find neighbours\n",
    "    X_test: dataset to find neighbors for\n",
    "    BallTree_leaf_size: leaf size of kd tree\n",
    "    radius: radius in high dimensional space to search for NNs\n",
    "    \n",
    "    Returns:\n",
    "    counts: count of NNs for each datapoint\n",
    "    indices: indices of NNs from dataset X_train\n",
    "    \"\"\"\n",
    "   \n",
    "    tree = BallTree(X_train, leaf_size=leaf_size) \n",
    "    counts = tree.query_radius(X_test, r=radius, count_only=True)\n",
    "    indices = tree.query_radius(X_test, r=radius)\n",
    "    return counts, indices\n",
    "\n",
    "def getVariablesCI(X,alpha):\n",
    "    \"\"\"\n",
    "    Method that computes the mean of the NN and then uses that mean to define an interval based on a normal distr.\n",
    "    Returns that interval.\n",
    "    X: data\n",
    "    alpha: value of C.I.\n",
    "    \"\"\"\n",
    "    if X.ndim > 1:\n",
    "        confs = []\n",
    "        X = X.T\n",
    "\n",
    "        for i in X:\n",
    "\n",
    "            mean, sigma,conf_int = confidenceInterval(X= i[~np.isnan(i)],alpha=alpha)\n",
    "            #mean, sigma = np.mean(X[indices,i]), np.std(X[indices,i])\n",
    "            #conf_int = stats.norm.interval(alpha, loc=mean, scale=sigma)\n",
    "            confs.append(conf_int)\n",
    "\n",
    "        return confs\n",
    "    else:\n",
    "        mean, sigma,conf_int = confidenceInterval(X=X[~np.isnan(X)],alpha=alpha)\n",
    "        return conf_int\n",
    "\n",
    "def getVariablesLI(X,alpha):\n",
    "    \"\"\"\n",
    "    Method that can be used to define a \"manual\" interval of a variable based on the initial value of the datapoint\n",
    "    X: data\n",
    "    alpha: interval range\n",
    "    \n",
    "    Example:\n",
    "    [1,2,3] if alpha  = 0.1 \n",
    "    [0.9,1.1] is the interval of the first variable\n",
    "    [1.9,2.1]\n",
    "    [2.9,3.1]\n",
    "    \"\"\"\n",
    "    \n",
    "    if not isinstance(X, list):\n",
    "        return (X-alpha,X+alpha)\n",
    "    else:\n",
    "        confs = []\n",
    "        for i in X.shape[0]:\n",
    "            conf_int = np.array([X[i]-alpha,X[:,i]+alpha]) # +- percentage of variable value\n",
    "            confs.append(conf_int)\n",
    "\n",
    "        return confs        \n",
    "\n",
    "def confidenceInterval(X,alpha,median=True):\n",
    "    \"\"\"\n",
    "    Method: that compute sthe C.I. of a normal distribution.\n",
    "    \n",
    "    X:data\n",
    "    \n",
    "    Returns:\n",
    "    mean: mean of distribution\n",
    "    sigma: standard deviation\n",
    "    conf_int: the confidence interval\n",
    "    \"\"\"\n",
    "    \n",
    "    if median:\n",
    "        median, sigma = np.median(X), np.std(X)\n",
    "        conf_int = stats.norm.interval(alpha, loc=median, scale=sigma)\n",
    "        return median, sigma, conf_int\n",
    "    else:\n",
    "        mean, sigma = np.mean(X), np.std(X)\n",
    "        conf_int = stats.norm.interval(alpha, loc=mean, scale=sigma)\n",
    "        return mean, sigma, conf_int\n",
    "\n",
    "# function to create all combinations of variables intervals\n",
    "def cartesian(arrays, out=None):\n",
    "    \"\"\"\n",
    "    Generate a cartesian product of input arrays.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    arrays : list of array-like\n",
    "        1-D arrays to form the cartesian product of.\n",
    "    out : ndarray\n",
    "        Array to place the cartesian product in.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    out : ndarray\n",
    "        2-D array of shape (M, len(arrays)) containing cartesian products\n",
    "        formed of input arrays.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> cartesian(([1, 2, 3], [4, 5], [6, 7]))\n",
    "    array([[1, 4, 6],\n",
    "           [1, 4, 7],\n",
    "           [1, 5, 6],\n",
    "           [1, 5, 7],\n",
    "           [2, 4, 6],\n",
    "           [2, 4, 7],\n",
    "           [2, 5, 6],\n",
    "           [2, 5, 7],\n",
    "           [3, 4, 6],\n",
    "           [3, 4, 7],\n",
    "           [3, 5, 6],\n",
    "           [3, 5, 7]])\n",
    "           \n",
    "    Obtained and validated from:\n",
    "    https://stackoverflow.com/questions/1208118/using-numpy-to-build-an-array-of-all-combinations-of-two-arrays\n",
    "    \"\"\"\n",
    "    \n",
    "    arrays = [np.asarray(x) for x in arrays]\n",
    "    dtype = arrays[0].dtype\n",
    "\n",
    "    n = np.prod([x.size for x in arrays])\n",
    "    if out is None:\n",
    "        out = np.zeros([n, len(arrays)], dtype=dtype)\n",
    "\n",
    "    m = int(n / arrays[0].size)\n",
    "    \n",
    "    out[:,0] = np.repeat(arrays[0], m)\n",
    "    if arrays[1:]:\n",
    "        cartesian(arrays[1:], out=out[0:m,1:])\n",
    "        for j in range(1, arrays[0].size):\n",
    "            out[j*m:(j+1)*m,1:] = out[0:m,1:]\n",
    "    return out\n",
    "\n",
    "def spaceSteps(step_size,confs):\n",
    "    \"\"\"\n",
    "    Method: to compute all intervals in a linear steps.\n",
    "    \n",
    "    step_size: is the the linear step size for each interval.\n",
    "    confs: is the set of confidence intervals to be used.\n",
    "    \n",
    "    Returns:\n",
    "    intervals: vectors of the intervals for each variable\n",
    "    \"\"\"\n",
    "    \n",
    "    for i in range(0,len(confs)):\n",
    "        conf_int = confs[i]\n",
    "        if i==0:\n",
    "            intervals = np.linspace(conf_int[0],conf_int[1],step_size)\n",
    "        else:\n",
    "            interval = np.linspace(conf_int[0],conf_int[1],step_size)\n",
    "            intervals = np.column_stack((intervals,interval))\n",
    "    \n",
    "    return intervals\n",
    "\n",
    "def getNNs(dfTrain, dfTest, combs, row, radius, printOutput, caseInd):\n",
    "    \n",
    "    \"\"\"\n",
    "    Method that identifies the similar cases of a test case and returns them.\n",
    "    dfTrain: training dataframe\n",
    "    dfTest: testing dataframe\n",
    "    combs: combinations of complete variables\n",
    "    row: boolean vector representing the missing variables\n",
    "    radius: radius of the hypersphere used by the BallTree algorithm\n",
    "    printOutput: boolean if True used to printhe indices and similar cases\n",
    "    caseInd: integer index of the test case\n",
    "    \n",
    "    Returns:\n",
    "    dfAllNNs: dataframe of Nearest Neighbors\n",
    "    allCounts: count of similar cases\n",
    "    allIndices: indices of all simialr cases\n",
    "    \"\"\"\n",
    "    \n",
    "    #######################################################################\n",
    "    #create dataframe to get similar cases with replacement\n",
    "    df = dfTrain.copy()\n",
    "    df.set_index('oldIndex',inplace=True)\n",
    "\n",
    "    #######################################################################\n",
    "    allCounts = []\n",
    "    allIndices = []\n",
    "    dfAllNNs = pd.DataFrame()\n",
    "\n",
    "    for i in combs:\n",
    "        ##### get dataframe of training\n",
    "        X_train = dfTrain[['oldIndex'] + i].dropna(axis=0)\n",
    "        cols = [k for k in X_train.columns if k not in list(dfTrain.columns[row])]\n",
    "        X_train = X_train[cols].as_matrix()\n",
    "        \n",
    "        if X_train.shape[1]>1: #case in which too many missing values\n",
    "            ##### get dataframe of testing\n",
    "            X_test = dfTest.loc[caseInd,cols].as_matrix()\n",
    "            \n",
    "            ##### normalize\n",
    "            scaler = MinMaxScaler()\n",
    "            scaler.fit(X_train[:,1:])\n",
    "\n",
    "            X_test[1:] = scaler.transform(X_test[1:].reshape(1,-1))\n",
    "\n",
    "            ##### get NNs\n",
    "            counts, indices = queryNN(X_train[:,1:],[X_test[1:]],radius=radius*len(i)*0.1,leaf_size=10)\n",
    "            allCounts.append(counts)\n",
    "            allIndices.append(indices)\n",
    "\n",
    "            ##### save NNs\n",
    "            dfTemp = df.loc[np.asarray(X_train[list(indices[0]),0], dtype=int),:]\n",
    "            dfTemp.reset_index(inplace=True) #reset index because index of df is oldIndex set up on top\n",
    "            dfAllNNs = dfAllNNs.append(dfTemp,ignore_index=True)\n",
    "        \n",
    "    if printOutput:\n",
    "        print('Number of NNs: ' , np.sum(allCounts))\n",
    "        print('Indices: ' ,allIndices)\n",
    "        \n",
    "    return dfAllNNs, allCounts, allIndices\n",
    "\n",
    "def booleanRow(columns, cols):\n",
    "    \"\"\"\n",
    "    Method that gets two string lists and checks if cols elements are in columns, returns boolean vector that represents the element\n",
    "    of columns in which the elemetn of cols is.\n",
    "    \n",
    "    columns: list of string\n",
    "    cols: lis of strings\n",
    "    \n",
    "    Returns:\n",
    "    Boolean list\n",
    "    \"\"\"\n",
    "    \n",
    "    boolRow = []\n",
    "    for i in columns:\n",
    "        if i in cols:\n",
    "            boolRow.append(True)\n",
    "        else:\n",
    "            boolRow.append(False)\n",
    "\n",
    "    return boolRow\n",
    "\n",
    "def getDatasetOfVariations(dfAllNNs,dfTest, row, caseInd, categorical, continuous, alpha, \n",
    "                           variations, partialLinear, linearVarCols):\n",
    "    \n",
    "    \"\"\"\n",
    "    Method that generates the dataset fo similar cases, called variations for the case of continuous variables as\n",
    "    all possible cases that fall insed the range of similar cases values are returned.\n",
    "    dfAllNNs: dataframe of all similar cases\n",
    "    dfTest: test set dataframe\n",
    "    row: boolean vector representing the variables with missing data as True\n",
    "    caseInd: index of test case\n",
    "    categorical: list of categorical variables\n",
    "    continuous: list of continuous variables\n",
    "    alpha: the confidence interval value\n",
    "    variations: boolean that represents whether to generate C.I.s for imputations\n",
    "    partialLinear: boolean that represents whether we generate simulated data points in uncertain values\n",
    "    linearVarCols: list of columns fo the variables to generate simulated data points in uncertain values\n",
    "    \n",
    "    Returns:\n",
    "    A numpy array of the imputed test case\n",
    "    \"\"\"\n",
    "\n",
    "    #######################################################################\n",
    "    \n",
    "    x = dfTest.loc[caseInd].as_matrix()\n",
    "       \n",
    "    if sum(row)>0: #if there are missing values\n",
    "        boolCategorical = booleanRow(dfAllNNs.columns,categorical)\n",
    "        boolContinuous = booleanRow(dfAllNNs.columns,continuous)\n",
    "\n",
    "        catColumns = np.logical_and(boolCategorical,row) #oldIndex not present in dfAllNNs\n",
    "        contColumns = np.logical_and(boolContinuous,row)\n",
    "                \n",
    "        if (np.sum(catColumns)>0): \n",
    "            cols = dfAllNNs.columns[catColumns]\n",
    "            freqValues = [dfAllNNs[i].value_counts().index[0] for i in cols]\n",
    "            ######## impute categorical values\n",
    "            ind = np.array(catColumns)\n",
    "            x[ind] = freqValues\n",
    "        if(np.sum(contColumns)>0):\n",
    "            cols = dfAllNNs.columns[contColumns]\n",
    "            if partialLinear:# and 'C_currentage' in cols:\n",
    "                confs = []\n",
    "                for j in cols:\n",
    "                    if j in linearVarCols and ~row[list(dfAllNNs.columns).index(j)]:\n",
    "                        confs.append(getVariablesLI(dfTest.loc[caseInd,j],alpha=1.0))\n",
    "                    else:\n",
    "                        confs.append(getVariablesCI(dfAllNNs[j].as_matrix(),alpha=alpha))\n",
    "                x = getVariations(x=x, variations=variations, contColumns=contColumns, confs=confs, step_size=10) \n",
    "            else:\n",
    "                confs = []\n",
    "                for j in cols:\n",
    "                    confs.append(getVariablesCI(dfAllNNs[j].as_matrix(),alpha=alpha))\n",
    "                x = getVariations(x=x, variations=variations, contColumns=contColumns, confs=confs, step_size=10)\n",
    "        else:\n",
    "            contColumns = booleanRow(dfAllNNs.columns,linearVarCols)\n",
    "            cols = dfAllNNs.columns[contColumns]\n",
    "            if partialLinear:# and 'C_currentage' in cols:\n",
    "                confs = []\n",
    "                for j in cols:\n",
    "                    if j in linearVarCols and ~row[list(dfAllNNs.columns).index(j)]:\n",
    "                        confs.append(getVariablesLI(dfTest.loc[caseInd,j],alpha=1.0))\n",
    "                x = getVariations(x=x, variations=variations, contColumns=contColumns, confs=confs, step_size=10) \n",
    "            \n",
    "                \n",
    "    return x\n",
    "\n",
    "\n",
    "def getDatasetOnlyVariationsLinear(dfTest, row, caseInd, categorical, continuous, \n",
    "                                    variations, partialLinear, linearVarCols):\n",
    "    \"\"\"\n",
    "    Method that generates simulated data points based on uncertain values of continuous cariables.\n",
    "    Input:\n",
    "    dfTest: test set dataframe\n",
    "    row: boolean vector representing the variables with missing data as True\n",
    "    caseInd: index of test case\n",
    "    categorical: list of categorical variables\n",
    "    continuous: list of continuous variables \n",
    "    variations: boolean that represents whether to generate C.I.s for imputations\n",
    "    partialLinear: boolean that represents whether we generate simulated data points in uncertain values\n",
    "    linearVarCols: list of columns fo the variables to generate simulated data points in uncertain values\n",
    "    \n",
    "    Returns:\n",
    "    numpy array with simulated data points (imputed data points)\n",
    "    \"\"\"\n",
    "    \n",
    "    x = dfTest.loc[caseInd].as_matrix()\n",
    "    \n",
    "    if sum(row)==0 and partialLinear:\n",
    "        cols = [i for i in dfTest.columns]\n",
    "        boolCategorical = booleanRow(cols,categorical)\n",
    "        boolContinuous = booleanRow(cols,continuous)\n",
    "\n",
    "        if partialLinear:\n",
    "            boolCols = booleanRow(cols,linearVarCols)\n",
    "            row = boolCols | row\n",
    "\n",
    "        catColumns = np.logical_and(boolCategorical,row) #oldIndex not present in dfAllNNs\n",
    "        contColumns = np.logical_and(boolContinuous,row)\n",
    "\n",
    "        confs = []\n",
    "        for j in linearVarCols:\n",
    "            confs.append(getVariablesLI(dfTest.loc[caseInd,j],alpha=1.0))\n",
    "        x = getVariations(x=x, variations=variations, contColumns=contColumns, confs=confs, step_size=10) \n",
    "         \n",
    "    return x\n",
    "\n",
    "def getVariations(x, variations, contColumns,confs,step_size):\n",
    "    \n",
    "    \"\"\"\n",
    "    Method that generates all variations of simulated data points by computing all combinations of imputed values.\n",
    "    \n",
    "    Input:\n",
    "    x: numpy array of a data point\n",
    "    variations: boolean that represents whether to generate C.I.s for imputations\n",
    "    contColumns: list of booleans that represents the index of continuous columns\n",
    "    confs: list of tuples that contains the range of values for each continuous variable\n",
    "    step_size: the number of steps in the conf range\n",
    "    \n",
    "    Returns:\n",
    "    x all variations of simulated data from imputed range of values.\n",
    "    \"\"\"\n",
    "    \n",
    "    if variations==True:\n",
    "        ########### Define intervals in linear steps\n",
    "        intervals = spaceSteps(step_size=step_size, confs=confs)\n",
    "        ########### All combinations of variables intervals\n",
    "        intsCombs = cartesian(intervals.T)\n",
    "\n",
    "        ind = np.array(contColumns)\n",
    "\n",
    "        if(intsCombs.shape[0] > 1):\n",
    "            x = np.tile(x, (intsCombs.shape[0],1))\n",
    "            x[:,ind] = intsCombs\n",
    "        else:\n",
    "            x = np.tile(x, (intsCombs.shape[1],1))\n",
    "            x[:,ind] = intsCombs.T\n",
    "    else:\n",
    "        confs_mean = [np.mean(i) for i in confs]\n",
    "        ind = np.array(contColumns)\n",
    "        x[ind] = confs_mean\n",
    "        \n",
    "    return x\n",
    "\n",
    "def getCombinations(row, df, tolerance_Value):\n",
    "    \"\"\"\n",
    "    Method: computes all the combinations of the remaining complete variables of the test set, it uses a tolerance value in each iteration\n",
    "    that represents the percentage of variables included to compute all of the possibel combinations.\n",
    "    \n",
    "    row: boolean row representing were there is a mising value in the variables\n",
    "    df: training dataset\n",
    "    tolerance_Value: tolerance value represents the tolerance to missing data\n",
    "    \n",
    "    Returns:\n",
    "    All the combinations of possible variables based on the tolerance\n",
    "    \"\"\"\n",
    "    \n",
    "    cols = ['MRN_D','G_5yearscore','oldIndex'] + list(df.columns[row])\n",
    "    arrays = [i for i in df.columns if i not in cols]\n",
    "    combs = []\n",
    "    for i in range(int(np.round(len(arrays)*tolerance_Value)),len(arrays)+1):\n",
    "        combs = combs + [list(x) for x in it.combinations(arrays, i)]\n",
    "    \n",
    "    return combs\n",
    "\n",
    "###################################################################################################################\n",
    "###################################################################################################################\n",
    "\n",
    "def train_test_split(df, testSetSize, extTestSetSize,external_validation, as_dataframe):\n",
    "    \"\"\"\n",
    "    Method that splits a dataset into random training and test set or random training, test and external test set\n",
    "    df: datafram that constins the data\n",
    "    testSetSize: defines the size of the test size\n",
    "    extTestSetSize: defines the size of the external test size\n",
    "    external_validation: boolean if true generates an external test set\n",
    "    as_dataframe: boolean if true returns training and test sets as pandas dataframes\n",
    "    \n",
    "    Returns:\n",
    "    The training and test set or the training, test and external test set.\n",
    "    \"\"\"\n",
    "    \n",
    "    test_ind = np.random.RandomState(200).choice(len(df), size= int(np.round(len(df)*testSetSize)),replace=False)\n",
    "    \n",
    "    df.reset_index(inplace=True)\n",
    "    df = df.rename(index=int,columns={'index':'oldIndex'})\n",
    "    train_ind = [i for i in range(len(df)) if i not in test_ind]\n",
    "    \n",
    "    if external_validation:\n",
    "        extTest_ind = np.random.RandomState(200).choice(len(train_ind), size= int(np.round(len(test_ind)*(1+extTestSetSize-testSetSize))),replace=False)\n",
    "        extTest_ind = [train_ind[i] for i in extTest_ind]\n",
    "        train_ind = [i for i in train_ind if i not in extTest_ind]\n",
    "        \n",
    "        cols = [i for i in df.columns]\n",
    "        if as_dataframe:\n",
    "            X_train = df.loc[train_ind,cols]\n",
    "            X_train.reset_index(inplace=True,drop=True)\n",
    "            X_test = df.loc[test_ind,cols]\n",
    "            X_test.reset_index(inplace=True,drop=True)\n",
    "            X_extTest = df.loc[extTest_ind,cols]\n",
    "            X_extTest.reset_index(inplace=True,drop=True)\n",
    "        else:\n",
    "            X_train = df.loc[train_ind,cols].as_matrix()\n",
    "            X_test = df.loc[test_ind,cols].as_matrix()\n",
    "            X_extTest = df.loc[extTest_ind,cols].as_matrix()\n",
    "            \n",
    "        return X_train, X_test, X_extTest\n",
    "    else:\n",
    "        cols = [i for i in df.columns]\n",
    "        if as_dataframe:\n",
    "            X_train = df.loc[train_ind,cols]\n",
    "            X_train.reset_index(inplace=True,drop=True)\n",
    "            X_test = df.loc[test_ind,cols]\n",
    "            X_test.reset_index(inplace=True,drop=True)\n",
    "        else:\n",
    "            X_train = df.loc[train_ind,cols].as_matrix()\n",
    "            X_test = df.loc[test_ind,cols].as_matrix()\n",
    "            \n",
    "        return X_train, X_test\n",
    "    \n",
    "def assignRandomValues(df,indexColumns,labelColumns,partialVarColumns,missingValues):\n",
    "    \"\"\"\n",
    "    Method that generates random missing values to a dataset\n",
    "    Inputs:\n",
    "    df: input data dataframe\n",
    "    indexColumns: columns that are indices not used in random generation of missing values\n",
    "    labelColumns: label columns not to be used in tandom gneeration\n",
    "    partialVarColumns: list of the columns that will undergo partial variability (range), so random missing values are \n",
    "                        not generated on these columns\n",
    "    missingValues: boolean indicating whether to introduce missing values or not\n",
    "    \n",
    "    Return:\n",
    "    Output:\n",
    "    df: dataframe with missing values\n",
    "    \"\"\"\n",
    "    if missingValues:\n",
    "\n",
    "        cols = [i for i in df.columns if i not in indexColumns + labelColumns]\n",
    "\n",
    "        rand = np.random.RandomState(200).randint(len(cols), size=(len(df),len(cols)))\n",
    "\n",
    "        for i in range(len(df)):\n",
    "            rand[i][rand[i]==np.random.RandomState(200).randint(len(cols))] = -1\n",
    "\n",
    "        indxs = np.where( rand < 0 )\n",
    "\n",
    "        dfMiss = df.copy()\n",
    "\n",
    "        for i,j in zip(indxs[0],indxs[1]):\n",
    "            dfMiss.iloc[i,j+len(indexColumns)] = np.nan\n",
    "\n",
    "        for i in partialVarColumns:\n",
    "            dfMiss[partialVarColumns] = df[partialVarColumns]\n",
    "\n",
    "        return dfMiss\n",
    "    else:\n",
    "        return df\n",
    "    \n",
    "###################################################################################################################\n",
    "###################################################################################################################    \n",
    "    \n",
    "def getRace(x):\n",
    "    \"\"\"\n",
    "    Method to impute race based on most frequent race\n",
    "    x: input dataframe\n",
    "    \n",
    "    Returns:\n",
    "    dataframe updated to include the variable race.\n",
    "    \"\"\"\n",
    "    ### picking the most frequent race\n",
    "    for i in tqdm_notebook(range(len(x))):\n",
    "        if x.loc[i,'white']==1:\n",
    "            x.loc[i,'race'] = 'white'\n",
    "\n",
    "        elif x.loc[i,'hispanic']==1:\n",
    "            x.loc[i,'race'] = 'hispanic'\n",
    "\n",
    "        elif x.loc[i,'black']==1:\n",
    "            x.loc[i,'race'] = 'black'\n",
    "\n",
    "        elif x.loc[i,'asian'] == 1:\n",
    "            x.loc[i,'race'] = 'asian'\n",
    "\n",
    "    #    elif x.loc[i,'amindian'] == 1:\n",
    "    #        x.loc[i,'race'] = 'white'\n",
    "\n",
    "        else:\n",
    "            x.loc[i,'race'] = 'white'\n",
    "    return x\n",
    "\n",
    "###################################################################################################################\n",
    "###################################################################################################################  \n",
    "\n",
    "def plotDistributionCI(X, theshold, ymax, bins):\n",
    "    \"\"\"\n",
    "    Method that plots the distribution of the probabilities and the C.I. and mean.\n",
    "    \n",
    "    X: data 1*d\n",
    "    threshold: threshold value\n",
    "    \n",
    "    Returns:\n",
    "    Plot of the distribution\n",
    "    \"\"\"\n",
    "    \n",
    "    plt.style.use('ggplot')\n",
    "\n",
    "    plt.figure(figsize=(12,6))\n",
    "    plt.hist(X,alpha=0.5,bins=bins,color='blue',label='5 year risk distribution')\n",
    "\n",
    "    plt.vlines( x=theshold,ymin=0, ymax=ymax, alpha=0.8, color='red', linewidth='3', label='Threshold')\n",
    "\n",
    "    plt.title('Distribution of Risk with 1.67 threshold')\n",
    "    plt.xlabel('5 Year Risk value')\n",
    "    plt.ylabel('density')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def plotDistributionCIRisk(X, ymax, bins):\n",
    "    \"\"\"\n",
    "    Method that plots the distribution of the probabilities and the C.I. and mean.\n",
    "    \n",
    "    X: data 1*d\n",
    "    threshold: threshold value\n",
    "    \n",
    "    Returns:\n",
    "    Plot of the distribution\n",
    "    \"\"\"\n",
    "\n",
    "    plt.style.use('ggplot')\n",
    "\n",
    "    plt.figure(figsize=(18,6))\n",
    "    plt.hist(X,alpha=0.5,bins=bins,color='blue',label='5 year risk distribution')\n",
    "    \n",
    "    mean, _, conf_int = confidenceInterval(X,alpha=0.95)\n",
    "    print('Mean: ', mean)\n",
    "    print('Median: ', np.median(X))\n",
    "    print('Number of risk values: ', len(X))\n",
    "    print('C.I.: ', conf_int)\n",
    "    plt.vlines( x=conf_int[0],ymin=0, ymax=ymax, alpha=0.8, color='green', linewidth='3',label='95% C.I. of median')\n",
    "    plt.vlines( x=conf_int[1],ymin=0, ymax=ymax, alpha=0.8, color='green', linewidth='3')\n",
    "    plt.vlines( x=np.median(X),ymin=0, ymax=ymax, alpha=0.8, color='black', linewidth='3'\n",
    "               , label= 'median = ' + str(np.round(mean,2)))\n",
    "    plt.vlines( x=1.67,ymin=0, ymax=ymax, alpha=0.8, color='red', linewidth='3', label='1.67 Threshold')\n",
    "    \n",
    "    plt.xticks(fontsize=14)\n",
    "    plt.yticks(fontsize=14)\n",
    "    plt.title('Distribution of Risk with 1.67 threshold', fontsize=20)\n",
    "    plt.xlabel('5 Year Risk value', fontsize=16)\n",
    "    plt.ylabel('density', fontsize=16)\n",
    "    plt.legend(loc='best', fontsize=16)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "44px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
